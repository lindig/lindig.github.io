@InProceedings{breu:06:etx,
  author =       "Silvia Breu and Thomas Zimmerman and Christian Lindig",
  title =        "HAM: Cross-Cutting Concerns in Eclipse",
  booktitle =    "Eclipse Technology eXchange (ETX) Workshop at OOPSLA 2006",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Michael G. Burke and Cheryl Morris and Allessando Orso
                  and Martin Robillard",
  OPTvolume =    "",
  OPTnumber =    "",
  OPTseries =    "",
  OPTorganization = "",
  publisher =    "ACM",
  address =      "Portland, OR, USA",
  OPTmonth =     oct,
  OPTnote =      "",
  OPTpages =     "",
  OPTprice =     "",
  bibdate =      "Thu Feb 10 16:53:46 1994",
}

@InProceedings{breu:06:oopsla:poster,
  author =       "Silvia Breu and Thomas Zimmermann and Christian
                 Lindig",
  title =        "Aspect Mining for Large Systems (Poster)",
  booktitle =    "ACM SIGPLAN Int.~Conf. on Object-Oriented Programming,
                 Systems, Languages, and Applications ({OOPSLA} 2006)",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Klaus Ostermann",
  OPTvolume =    "",
  OPTnumber =    "",
  OPTseries =    "",
  OPTorganization = "",
  publisher =    "ACM",
  address =      "Portland, OR, USA",
  OPTmonth =     "",
  OPTnote =      "",
  OPTpages =     "",
  OPTprice =     "",
  bibdate =      "Thu Feb 10 16:53:46 1994",
}

@InProceedings{breu:06:oopsla:demo,
  author =       "Silvia Breu and Thomas Zimmermann and Christian
                 Lindig",
  title =        "Aspect Mining for Large Systems (Demo)",
  booktitle =    "ACM SIGPLAN Int.~Conf. on Object-Oriented Programming,
                 Systems, Languages, and Applications ({OOPSLA} 2006)",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Yvonne Coady",
  OPTvolume =    "",
  OPTnumber =    "",
  OPTseries =    "",
  OPTorganization = "",
  publisher =    "ACM",
  address =      "Portland, OR, USA",
  OPTmonth =     "",
  OPTnote =      "",
  OPTpages =     "",
  OPTprice =     "",
  bibdate =      "Thu Feb 10 16:53:46 1994",
}
@InProceedings{breu:06:wsr,
  author =       "Silvia Breu and Thomas Zimmerman and Christian Lindig",
  title =        "Mining Aspects from CVS Transactions using Concept Analysis",
  booktitle =    "8. Workshop Software-Reengineering",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Rainer Giminich and Volker Riediger and Andreas Winter",
  OPTvolume =    "",
  OPTnumber =    "",
  OPTseries =    "",
  OPTorganization = "",
  OPTpublisher = "",
  address =      "Bad Honnef, Germany",
  month =        may,
  OPTnote =      "",
  OPTpages =     "",
  OPTprice =     "",
  pdf =
  "http://www.st.cs.uni-sb.de/~lindig/papers/ham/papers/WSR/2006/wsr2006_2.pdf",
  note =     "We identify cross-cutting functionality by analyzing 
                  the version history of a project. The (co-)addition of the
                  same set of method calls in many unrelated methods
                  point to a cross-cutting concern. We succesfully analyze CVS
                  transaction for such co-additions using formal concept
                  analysis. The paper presents from results from
                  analyzing Eclipse, a 1.6 MLOC Java application.", 
  bibdate =      "Thu Feb 10 16:53:46 1994",
}



@Manual{ramsey:05:cmm,
  title =        "The C-- Language Specification Version 2.0",
  author =       "Norman Ramsey and Simon Peyton Jones and Christian Lindig",
  organization = "Harvard University",
  address =      "Cambridge, Massachussets, USA",
  year =         "2005",
  month =         jan,
  note =     "Specification of the portable assembly language C--",
  pdf =          "http://www.cminusminus.org/extern/man2.pdf",
  bibdate =      "Thu Feb 10 16:53:17 1994",
}


@InProceedings{dallmeier:06:woda,
  author =       "Valentin Dallmeier and Christian Lindig and Andrzej
                 Wasylkowski and Andreas Zeller",
  title =        "Mining Object Behavior with {ADABU}",
  booktitle =    "WODA 2006: ICSE Workshop on Dynamic Analysis",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Neelam Gupta and Andy Podgurski",
  address =      "Shanghai, China",
  month =        may,
  pdf = "http://www.st.cs.uni-sb.de/~lindig/papers/ample/papers/issta2006/draft.pdf",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  note =     "To learn what constitutes correct program behavior,
                 one can start with normal behavior. We observe actual
                 program executions to construct state machines that
                 summarize object behavior. These state machines, called
                 object behavior models, capture the relationships
                 between two kinds of methods: mutators that change the
                 state (such as add()) and inspectors that keep the
                 state unchanged (such as isEmpty()): ``A Vector object
                 initially is in isEmpty() state; after add(), it goes
                 into not isEmpty() state''. Our ADABU prototype for
                 JAVA has successfully mined models of undocumented
                 behavior from the AspectJ compiler and the Columba
                 email client; the models tend to be small and easily
                 understandable.",
}

@InProceedings{zimmerman:06:msrc,
  author =       "Thomas Zimmerman and Silvia Breu and Christian Lindig
                 and Benjamin Livshits",
  title =        "Mining Additions of Method Calls in Argo{UML} (Mining
                 Challenge)",
  booktitle =    "International Workshop on Mining Software
                 Repositories",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Stephan Diehl and Harald Gall and Ahmed E. Hassan",
  address =      "Shanghai, China",
  month =        may,
  note =         "accepted for publication",
  note =     "In this paper we refine the classical co-change to the
                 addition of method calls. We use this concept to find
                 usage patterns and to identify cross-cutting concerns
                 for ARGOUML",
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/papers/ham/papers/msr2006challenge/camera-ready/MSR04c-zimmermann.pdf",
  bibdate =      "Thu Feb 10 16:53:46 1994",
}
@InProceedings{breu:06:msr,
  author =       "Silvia Breu and Thomas Zimmerman and Christian
                 Lindig",
  title =        "Mining {Eclipse} for Cross-Cutting Concerns",
  booktitle =    "International Workshop on Mining Software Repositories
                 (MSR)",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Stephan Diehl and Harald Gall and Ahmed E. Hassan and
                 Martin Pinzger",
  address =      "Shanghai, China",
  month =        may,
  pdf =  "http://www.st.cs.uni-sb.de/~lindig/papers/ham/papers/msr2006/msr.pdf",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  note =     "Software may contain functionality that does not align
                 with its architecture. Such cross-cutting concerns do
                 not exist from the beginning but emerge over time. By
                 analysing where developers add code to a program, our
                 history-based mining identifies cross-cutting concerns
                 in a two-step process. First, we mine CVS archives for
                 sets of methods where a call to a specific single
                 method was added. In a second step, simple
                 cross-cutting concerns are combined to complex
                 cross-cutting concerns. To compute these efficiently,
                 we apply formal concept analysis---an algebraic theory.
                 Unlike approaches based on static or dynamic analysis,
                 history-based mining for cross-cutting concerns scales
                 to industrial-sized projects: For example, we
                 identified a locking concern that cross-cuts 1284
                 methods in the open-source project ECLIPSE.",
}

@InProceedings{olinsky:06:popl,
  author =       "Reuben Olinksy and Christian Lindig and Norman
                 Ramsey",
  title =        "Staged Allocation: {A} Compositional Technique for
                 Specifying and Implementing Procedure Calling
                 Conventions",
  booktitle =    "Conference Record of the 33rd Annual ACM Symposium on
                 Principles of Programming Languages",
  year =         "2006",
  ISBN =         "",
  ISBN =         "",
  editor =       "Greg Morrisett and Simon Pyton Jones",
  publisher =    "ACM",
  address =      "Charleston, SC, USA",
  month =        jan,
  pages =        "409--421",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  pdf = "http://www.st.cs.uni-sb.de/~lindig/papers/popl-06/popl-06.pdf",
  note =     "We present staged allocation, a technique for
                 specifying calling conventions by composing tiny
                 allocators called stages. A specification written using
                 staged allocation has a precise, formal semantics, and
                 it can be executed directly inside a compiler.
                 Specifications of nine standard C calling conventions
                 range in size from 15 to 30 lines each. An
                 implementation of staged allocation takes about 250
                 lines of ML or 650 lines of C++. Each specification can
                 be used not only to help a compiler implement the
                 calling convention but also to generate a test suite.",
}

@Article{lindig:05:ix,
  author =       "Christian Lindig",
  title =        "Schwachstellensucher",
  journal =      "iX -- Magazin für professionelle Informationstechnik",
  year =         "2005",
  volume =       "05",
  number =       "09",
  pages =        "150--153",
  month =        aug,
  note =     "C-Compiler müssen Funktionsaufrufe in Maschinencode
                 übersetzen, der komplizerten Aufrufkonventionen folgt.
                 Dabei machen auch ausgereifte Compiler Fehler, wie der
                 Testfallgenerator Quest aufdeckt.",
  bibdate =      "Thu Feb 10 16:53:42 1994",
}

@InProceedings{dallmeier:05:aadebug,
  author =       "Valentin Dallmeier and Christian Lindig and Andreas
                 Zeller",
  title =        "Lightweight Bug Localization with {AMPLE} (Demo
                 Paper)",
  booktitle =    "Sixth International Symposium on Automated and
                 Analysis-Driven Debugging (AADEBUG)",
  year =         "2005",
  ISBN =         "",
  editor =       "Jong Deok Choi and Raimondas Lencevicius",
  publisher =    "ACM Press",
  address =      "Monterey, CA, USA",
  month =        sep,
  pages =        "99--103",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/papers/ample/ample.pdf",
  note =     "AMPLE locates likely failure-causing classes by
                 comparing method call sequences of passing and failing
                 runs. A difference in method call sequences, such as
                 multiple deallocation of the same resource, is likely
                 to point to the erroneous class. Such sequences can be
                 collected from arbitrary Java programs at low cost;
                 comparing object-specific sequences predicts defects
                 better than simply comparing coverage. AMPLE comes as
                 an Eclipse plug-in that is automatically invoked as
                 soon as a JUnit test fails.",
}

@InProceedings{lindig:05:aadebug,
  author =       "Christian Lindig",
  title =        "Random Testing of {C} Calling Conventions",
  booktitle =    "Sixth International Symposium on Automated and
                 Analysis-Driven Debugging (AADEBUG)",
  year =         "2005",
  ISBN =         "",
  editor =       "Jong Deok Choi and Raimondas Lencevicius",
  publisher =    "ACM Press",
  address =      "Monterey, CA, USA",
  month =        sep,
  pages =        "3--11",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/src/quest/quest.pdf",
  note =     "In a C compiler, function calls are difficult to
                 implement correctly because they must respect a
                 platform-specific calling convention. But they are
                 governed by a simple invariant: parameters passed to a
                 function must be received unaltered. A violation of
                 this invariant signals an inconsistency in a compiler.
                 We automatically test the consistency of C compilers
                 using randomly generated programs. An inconsistency
                 manifests itself as an assertion failure when compiling
                 and running the generated code. The generation of
                 programs is type-directed and can be controlled by the
                 user with composable random generators in about 100
                 lines of Lua. Lua is a scripting language built into
                 our testing tool that drives program generation. Random
                 testing is fully automatic, requires no specification,
                 yet is comparable in effectiveness with
                 specification-based testing from prior work. Using this
                 method, we uncovered 13 new bugs in mature open-source
                 and commercial C compilers.",
}

@InProceedings{dallmeier:05:bugs,
  author =       "Valentin Dallmeier and Christian Lindig and Andreas
                 Zeller",
  title =        "Evaluating a Lightweight Defect Localization Tool",
  booktitle =    "PLDI Workshop on the Evaluation of Software Defect
                 Detection Tools",
  year =         "2005",
  ISBN =         "",
  ISBN =         "",
  editor =       "Bill Pugh and Jim Larus",
  address =      "Chicago",
  month =        jun,
  note =         "Proceedings only distributed at workshop; to appear",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/papers/bugs-05/bugs.pdf",
  note =     "AMPLE locates likely failure-causing classes by
                 comparing method call sequences of passing and failing
                 runs. A difference in method call sequences, such as
                 multiple deallocation of the same resource, is likely
                 to point to the erroneous class. In this paper, we
                 describe the implementation of AMPLE as well as its
                 evaluation.",
}

@Article{lindig:05:wsr,
  author =       "Christian Lindig and Valentin Dallmeier and Andreas
                 Zeller",
  title =        "7th Workshop Software Reengineering: Lightweight
                 Control-Flow Abstraction",
  journal =      "Softwaretechnik-Trends",
  volume =       "25",
  number =       "2",
  pages =        "41--42",
  publisher =    "Gesellschaft für Informatik",
  year =         "2005",
  month =        may,
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/papers/wsre-05/wsre-05.pdf",
  note =     "To dynamically capture an application's control flow,
                 we propose call sequence sets as a light-weight
                 abstraction. Unlike prior approaches based on grammars
                 or trees, call sequence sets are easy to aggregate and
                 compare, which facilitates incremental construction and
                 comparison of control flow within and across program
                 runs.",
}

@InProceedings{dallmeier:05:ecoop,
  author =       "Valentin Dallmeier and Christian Lindig and Andreas
                 Zeller",
  title =        "Lightweight Defect Localization for Java",
  booktitle =    "Proceedings of 19th European Conference on
                 Object-Oriented Programming, ECOOP 2005",
  year =         "2005",
  ISBN =         "",
  editor =       "Andrew P. Black",
  number =       "3586",
  series =       "Lecture Notes in Computer Science",
  publisher =    "Springer",
  address =      "Glasgow, UK",
  month =        jul,
  pages =        "528--550",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  pdf =          "http://www.st.cs.uni-sb.de/papers/dlz2004/dlz2004.pdf",
  note =     "A common method to localize defects is to compare the
                 coverage of passing and failing program runs: A method
                 executed only in failing runs, for instance, is likely
                 to point to the defect. Some failures, though, come to
                 be only through a specific sequence of method calls,
                 such as multiple deallocation of the same resource.
                 Such sequences can be collected from arbitrary Java
                 programs at low cost; comparing object-specific
                 sequences predicts defects better than simply comparing
                 coverage. In a controlled experiment, our technique
                 pinpointed the defective class in 39\% of all test
                 runs.",
}

@InProceedings{lindig:05:seuh,
  author =       "Christian Lindig and Andreas Zeller",
  title =        "Ein Software-Praktikum als Sommerkurs",
  booktitle =    "Software Engineering im Unterricht der Hochschulen
                 (SEUH 9)",
  year =         "2005",
  ISBN =         "",
  editor =       "Klaus-Peter Löhr and Horst Lichter",
  publisher =    "dPunkt Verlag",
  address =      "Aachen, Germany",
  month =        feb,
  pages =        "68--80",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/papers/seuh2005.pdf",
  note =     "Ein semesterbegleitendes Softwaretechnik-Praktikum
                 verleitet die Teilnehmer dazu, vor lauter Begeisterung
                 und Gruppendruck andere Veranstaltungen zu
                 vernachlässigen. An der Universität des Saarlandes wird
                 das Praktikum daher in der vorlesungsfreien Zeit
                 absolviert, und zwar als 6-wöchiger Vollzeitkurs mit
                 begleitender Vorlesung. Diese Form wirkt
                 studienverkürzend und vereinfacht die Teamarbeit durch
                 Ganztags-Präsenz. Risikomindernde Maßnahmen wie ein
                 einheitliches, vorgegebenes Pflichtenheft, spielerische
                 Elemente und automatisch testbare Erfolgskriterien
                 sorgen für hohe Motivation bei reibungslosem Ablauf.",
}

@InProceedings{lindig:04:cc,
  author =       "Christian Lindig and Norman Ramsey",
  title =        "Declarative Composition of Stack Frames",
  booktitle =    "Proc. of the 14th International Conference on Compiler
                 Construction",
  year =         "2004",
  month =        apr,
  editor =       "Evelyn Duesterwald",
  number =       "2985",
  series =       "Lecture Notes in Computer Science",
  publisher =    "Springer",
  pages =        "298--312",
  bibdate =      "Thu Feb 10 16:53:46 1994",
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/papers/blocks/next.pdf",
  note =     "When a compiler translates a procedure, it must lay
                 out the procedure's stack frame in a way that respects
                 the calling convention. Many compilers respect the
                 convention either by restricting the order in which
                 slots can be allocated or by using different
                 abstractions ad hoc for allocating in different regions
                 of a frame. Such techniques cause the implementation of
                 the layout to be spread over much of the compiler,
                 making it difficult to maintain and verify. We have
                 concentrated the implementation of layout into a
                 single, unifying abstraction: the block. The block
                 abstraction decouples layout from slot allocation.
                 Stack-frame layout is specified in one central place,
                 and even complex layouts are achieved by composing
                 blocks using only two simple operators. Our
                 implementation is used in the Quick C-- compiler to
                 support multiple calling conventions on multiple
                 architectures.",
}

@InProceedings{ramsey:03:popl,
  author =       "Norman Ramsey and Christian Lindig",
  title =        "Custom Calling Conventions in a Portable Assembly
                 Language",
  booktitle =    "ACM SIGPLAN Programming Language Design and
                 Implementation",
  year =         "2003",
  month =        jan,
  publisher =    "ACM",
  note =         "rejected",
  bibdate =      "Thu Feb 10 16:53:46 1994",
}

@Unpublished{lindig:2000:pretty,
  author =       "Christian Lindig",
  title =        "Strictly Pretty",
  note =         "",
  year =         "2000",
  note =     "Pretty printers are tools for formatting structured
                 text. A recently taken algebraic approach has lead to a
                 systematic design of pretty printers. Wadler has
                 proposed such an algebraic pretty printer together with
                 an implementation for the lazy functional language
                 Haskell. The original design causes exponential
                 complexity when literally used in a strict language.
                 This note recalls some properties of Wadler's pretty
                 printer on an operational level and presents an
                 efficient implementation for the strict functional
                 language Objective Caml.",
}

@InProceedings{lindig:2000:fca,
  author =       "Christian Lindig",
  title =        "Fast Concept Analysis",
  booktitle =    "Working with Conceptual Structures - Contributions to
                 ICCS 2000",
  year =         "2000",
  month =        aug,
  ISBN =         "ISBN 3-8265-7669-1",
  editor =       "Gerhard Stumme",
  publisher =    "Shaker Verlag",
  address =      "Aachen, Germany",
  pages =        "152--161",
  pdf = "http://www.st.cs.uni-sb.de/~lindig/papers/fast-ca/iccs-lindig.pdf",
  note =     "Formal concept analysis is increasingly used for large
                 contexts that are built by programs. This paper
                 presents an efficient algorithm for concept analysis
                 that computes concepts together with their explicit
                 lattice structure. An experimental evaluation uses
                 randomly generated contexts to compare the running time
                 of the presented algorithm with two other algorithms.
                 Running time increases quadratically with the number of
                 concepts, but with a small quadratic component. At
                 least contexts with sparsely filled context tables
                 cause concept lattices grow quadratically with respect
                 to the size of their base relation. The growth rate is
                 controlled by the density of context tables. Modest
                 growth combined with efficient algorithms leads to fast
                 concept analysis.",
}

@PhdThesis{lindig:99:phd,
  author =       "Christian Lindig",
  title =        "Algorithmen zur Begriffsanalyse und ihre Anwendung bei
                 Softwarebibliotheken",
  school =       "Technische Universität Braunschweig",
  year =         "1999",
  address =      "D--38106 Braunschweig, Germany",
  month =        nov,
  pdf = "http://www.st.cs.uni-sb.de/~lindig/papers/dissertation/lindig-a4.pdf",
  note =     "Formal concept analysis is an algebraic theory
                 concerning binary relations and closely related
                 complete lattices of so-called concepts. This thesis
                 presents algorithms and data structures to compute
                 concepts and their lattice structure. Since, in the
                 worst case, the number of concepts can grow
                 exponentially with the size of a relation, the
                 complexity of algorithms was given special attention.
                 The speed of algorithms as well as the actual size of
                 concept lattices was tested with a large number of test
                 cases. They show that for practical applications the
                 performance of algorithms and the number of concepts
                 depends only quadratically on the size of the relation.
                 As an application for concept analysis the organization
                 of a library of re-usable software components is
                 proposed. Component based software reuse aims to raise
                 software quality and development productivity by
                 re-using already successfully developed components. The
                 proposed method combines good maintainability of the
                 component library with strong navigation support for
                 the user. The search tool uses the concept lattice as a
                 data structure that is computed once for the library.
                 The lattice essentially contains all decisions a user
                 can take while searching for a component and thus
                 supports efficient searching. Additionally the lattice
                 permits reasoning about the quality of the indexing
                 method used on the components.",
}

@InCollection{lindig:99:base,
  author =       "Christian Lindig and Gregor Snelting",
  title =        "Formale Begriffsanalyse im Software Engineering",
  booktitle =    "Begriffliche Wissensverarbeitung. Methoden und
                 Anwendungen",
  publisher =    "Springer",
  pdf = "http://www.st.cs.uni-sb.de/~lindig/papers/wille-stumme/wille.pdf",
  year =         "1999",
  month =        jan,
  ISBN =         "3-540-66391-6",
  editor =       "Gerhard Stumme and Bernhard Wille",
  note =     "Reuse und Reengineering sind aktuelle Probleme im
                 Software-Engineering. Reuse zielt auf die
                 Wiederverwendung von Software-Komponenten oder
                 -Schablonen aus einer Bibliothek; dazu ist es
                 notwendig, effektive Verfahren zur Komponentensuche
                 bereitzustellen. Reengineering befaßt sich mit der
                 Rekonstruktion von Systemstrukturen aus alter Software;
                 Ziel ist es, Altsoftware so zu transformieren, daß sie
                 auch weiterhin lebensfähig bleibt. Wir präsentieren
                 zwei Werkzeuge zur Unterstützung von Reuse und
                 Reengineering, die auf formaler Begriffsanalyse
                 basieren. RECS rekonstruiert Konfigurationsstrukturen
                 aus Quelltexten und stellt sie graphisch dar. Man
                 erhält bemerkenswerte Einsichten in die Struktur des
                 Konfigurationsraums: alle Abhängigkeiten zwischen
                 Konfigurationspfaden werden dargestellt, und
                 unerwünschte Interferenzen zwischen Konfigurationen
                 werden aufgedeckt. FOCS bietet interaktive,
                 inkrementelle Unterstützung zur Suche in
                 Software-Komponentenbibliotheken, in denen die
                 einzelnen Komponenten mit Attributen indiziert sind.",
}

@Unpublished{lindig:99:tkconcept,
  author =       "Christian Lindig",
  title =        "A Concept Analysis Framework",
  note =         "",
  year =         "1998",
  note =     "Concept analysis finds its way into applications, but
                 support for software developers is lacking: programmers
                 must still concentrate on concept analysis techniques
                 instead of application domains. TkConcept is a
                 framework for concept analysis applications that offers
                 domain independent support for concept analysis tasks.
                 It is implemented as an extension of Tcl/Tk and
                 provides abstractions for binary relations and concept
                 lattices. Using this framework, concept based
                 applications can be developed easier than with existing
                 tools.",
}

@TechReport{lindig:98,
  author =       "Christian Lindig",
  title =        "Analyse von Softwarevarianten",
  institution =  "TU Braunschweig, Institut für Programmiersprachen und
                 Informationssysteme, Abt. Softwaretechnologie",
  year =         "1998",
  type =         "Informatik-Bericht",
  number =       "98-04",
  address =      "D-38106 Braunschweig",
  month =        jan,
  note =     "Software-Quelltexte werden oft durch den Einsatz eines
                 Präprozessors an verschiedenen Zielplattformen
                 angepaßt. Aus einem Quelltext entstehen dabei durch den
                 Präprozessor verschiedene Varianten der Software, die
                 einen Variantenverband bilden. Formale Begriffsanalyse
                 ist eine mathematische Theorie, mit deren Hilfe der
                 Variantenverband von Quelltexten effizient bestimmt
                 werden kann. Darüberhinaus können Redundanzen in der
                 Beschreibung der Variantenstruktur entdeckt und
                 entfernt werden.",
}

@InProceedings{lindig:97:icse,
  author =       "Christian Lindig and Gregor Snelting",
  title =        "Assessing Modular Structure of Legacy Code Based on
                 Mathematical Concept Analysis",
  booktitle =    "Proceedings of the 1997 International Conference on
                 Software Engineering",
  publisher =    "ACM Press",
  year =         "1997",
  month =        may,
  ISBN =         "0-89791-914-9",
  pages =        "349--359",
  pdf =          "http://www.acm.org/pubs/articles/proceedings/soft/253228/p349-lindig/p349-lindig.pdf",
  note =     "We apply mathematical concept analysis in order to
                 modularize legacy code. By analysing the relation
                 between procedures and global variables, a so-called
                 concept lattice is constructed. The paper explains how
                 module structures show up in the lattice, and how the
                 lattice can be used to assess cohesion and coupling
                 between module candidates. Certain algebraic
                 decompositions of the lattice can lead to automatic
                 generation of modularization proposals. The method is
                 applied to several examples written in Modula-2,
                 Fortran, and Cobol; among them a >100 kloc aerodynamics
                 program.",
}

@InProceedings{lindig:95:softec,
  author =       "Christian Lindig",
  title =        "Komponentensuche mit Begriffen",
  booktitle =    "Softwaretechnik '95",
  address =      "Braunschweig",
  year =         "1995",
  month =        oct,
  pdf = "http://www.st.cs.uni-sb.de/~lindig/papers/softec-95/softec.pdf",
  note =     "Mit Schlüsselwörtern indexierte Komponenten einer
                 Sammlung werden einmalig durch formale Begriffsanalyse
                 zu einem Begriffsverband strukturiert. Dieser erlaubt
                 dann, Komponenten durch die Angabe von Schlüsselwörtern
                 einfach, effizient, inkrementell und mit Unterstützung
                 für den Benutzer zu suchen. Der Suchproze\ss{}
                 garantiert dabei, da\ss{} bei einer Suche mindestens
                 eine Komponente gefunden wird. Da der Suchproze\ss{}
                 unabhängig vom Inhalt der verwalteten Komponenten ist,
                 ist er auch für inhomogene und multimediale Sammlungen
                 geeignet. Eine prototypische Implementierung zur
                 Verwaltung einer Unix Online-Dokumentation belegt die
                 Anwendbarkeit des Verfahrens, dessen Komplexität
                 zusätzlich experimentell untersucht wurde. Es zeigt
                 sich, da\ss{} das Verfahren eine schnelle und genaue
                 Navigation in Sammlungen mit mehreren tausend
                 Komponenten ermöglicht.",
}

@InProceedings{lindig:95:ijcai,
  author =       "Christian Lindig",
  title =        "Concept-Based Component Retrieval",
  editor =       "Jana Köhler and Fausto Giunchiglia and Cordell Green
                 and Christoph Walther",
  pages =        "21--25",
  booktitle =    "Working Notes of the IJCAI-95 Workshop: Formal
                 Approaches to the Reuse of Plans, Proofs, and
                 Programs",
  year =         "1995",
  address =      "Montre\'al",
  month =        aug,
  pdf =          "http://www.st.cs.uni-sb.de/~lindig/papers/ijcai-95/ijcai.pdf",
  note =     "Reusable software components from a library are
                 individually indexed with a set of keywords. To
                 retrieve components the user incrementally specifies a
                 set of keywords that the searched components are
                 required to have. After each step the selected
                 components and the exact set of remaining significant
                 keywords needed to refine the query further are
                 presented to the user. The process ensures that at
                 least one component is found and the user cannot
                 specify conflicting keywords. The efficient computation
                 of retrieved components and significant keywords is
                 based on the precalculated concepts of the library,
                 which are natural pairs of component and keyword sets.
                 The concepts form a lattice of super- and subconcepts
                 and are obtained by formal concept analysis of the
                 relation over components and keywords. The two main
                 theorems state how to calculate the result of a query
                 and the remaining significant keywords using the
                 concept lattice. An implementation of the proposed
                 approach shows that the user can select components
                 quickly and precisely.",
}

@TechReport{lindig:94:ib,
  author =       "Christian Lindig",
  title =        "Inkrementelle, rückgekoppelte Suche in
                 Software-Bibliotheken",
  institution =  "Technische Universität Braunschweig, Institut für
                 Programmiersprachen und Informationssysteme, Abteilung
                 Softwaretechnologie",
  year =         "1994",
  type =         "Informatik-Bericht",
  number =       "94-07",
  address =      "Gaußstraße 17,D-38106 Braunschweig",
  month =        nov,
  note =     "Wiederverwendung von Software-Dokumenten verlangt eine
                 Ablage, in der relevante Dokumente leicht aufgefunden
                 werden können. Dokumente werden zur Suche in einer
                 Sammlung durch Schlüsselwörter unabhängig von einander
                 indexiert. Dokumente und Schlüsselwörter bilden
                 zusammen einen formalen Kontext, der einen
                 Begriffsverband impliziert. Der Begriffsverband stellt
                 eine natürliche, nur aus der Indexierung abgeleiteten
                 Gruppierung von Dokumenten und Attributen dar und
                 erlaubt die effiziente inkrementelle Suche mit
                 kontextsensitiver Unterstützung für den Benutzer. Die
                 verschiedenen Operationen werden formal und anhand
                 eines Beispiels präsentiert. Für eine Implementierung
                 wird eine günstige Repräsentation des Begriffsverbandes
                 vorgeschlagen und die typische Komplexität durch
                 Experimente untersucht. Abschließend wird das
                 präsentierte Verfahren mit verwandten Ansätzen
                 verglichen.",
}

@MastersThesis{lindig:93:style,
  author =       "Christian Lindig",
  title =        "Style---ein Typ-Checker für Scheme",
  school =       "Technische Universität Braunschweig, Institut für
                 Programmiersprachen und Informationssysteme,
                 Arbeitsgruppe Softwaretechnologie",
  year =         "1993",
  address =      "D-38106 Braunschweig, Gaußstraße 17, Germany",
  month =        sep,
  note =         "In German",
  note =     "This thesis describes an new tool for finding errors
                 in R4RS-compliant Scheme programs. A polymorphic type
                 system in the style of Milner with an additional
                 maximum type is used to type Scheme code. Although
                 Scheme is dynamically typed, most parts of programs are
                 statically typeable; type inconsistencies are regarded
                 as hints to possible programming errors. The paper
                 first introduces a type system which is a careful
                 balance between rigorous type safety and pragmatic type
                 softness. An efficient and portable implementation
                 based on order sorted unification in Scheme is then
                 described. We obtained very satisfactory results on
                 realistic programs, including the programs in
                 'Structure and Interpretation of Computer Programs' by
                 Abelson and Sussman.",
}

@TechReport{lindig:93:ib:style,
  author =       "Christian Lindig",
  title =        "Style---{A} practical Type Checker for Scheme",
  institution =  "Technische Universität Braunschweig, Institut für
                 Programmiersprachen und Informationssysteme,
                 Arbeitsgruppe Softwaretechnologie",
  year =         "1993",
  type =         "Informatik-Bericht",
  number =       "93-10",
  address =      "Gaußstraße 17, D-38106 Braunschweig",
  month =        oct,
  note =     "This paper describes an new tool for finding errors in
                 R4RS-compliant Scheme programs. A polymorphic type
                 system in the style of Damas and Milner (1982) with an
                 additional maximum type is used to type Scheme code.
                 Although Scheme is dynamically typed, most parts of
                 programs are statically typeable; type inconsistencies
                 are regarded as hints to possible programming errors.
                 The paper first introduces a type system which is a
                 careful balance between rigorous type safety and
                 pragmatic type softness. An efficient and portable
                 imple- mentation based on order sorted unification in
                 Scheme is then described. We obtained very satisfactory
                 results on realistic programs, including the programs
                 in Abelson, Sussman, and Sussman (1985).",
}

@Misc{lindig:cam:92,
  author =       "Christian Lindig",
  title =        "Die Kategorielle Abstrakte Maschine",
  howpublished = "Seminararbeit zur Implementierung Moderner
                 Programmiersprachen an der TU Braunschweig, Institut
                 für Programmiersprachen",
  year =         "1992",
  month =        jun,
}

@Article{lindig:cbr:2001,
  author =       "Anke Franzke and Jan Buer and Michael Probst-Kepper
                 and Christian Lindig and Marcus Franzke and Andres Jan
                 Schrader and Arnold Ganser and Jens Atzpodien",
  title =        "{HLA} Phenotype and Cytokine-Induced Tumor Control in
                 Advanced Renal Cell Cancer",
  journal =      "Cancer Biotherapy and Radiopharmaceuticals",
  year =         "2001",
  volume =       "16",
  number =       "5",
  pages =        "401--409",
  month =        oct,
  note =         "",
  note =     " 
    Background: The natural history of malignancies, the response to
    cytokine-based therapy and survival of patients may be partly determined
    by the human leukocyte antigen (HLA) phenotype. Here, we investigated in
    a retrospective analysis the correlation of the HLA phenotype of 73
    prognostic favored patients with advanced renal cell carcinoma to (a)
    the expected HLA distribution in Caucasians, (b) the susceptibility or
    resistance to metastatic sites, (c) response to cytokine-based therapy
    and (d) sustained cytokine-induced effective tumor control.

    Methods: We retrospectively determined the MHC class I and II antigens
    in patients with metastatic renal cell carcinoma selected by survival.
    Antigens were serologically typed by standard lymphocytotoxicity
    techniques. For statistical analysis, we calculated the probability of
    the presented HLA antigens in correlation to the expected Caucasian HLA
    phenotypes. An independent confirmation was performed by using the
    chi-square and two-tailed Fisher's exact test.

    Results: Various HLA antigens deviated significantly from the normal
    distribution in the Caucasian population. HLA.B44 was the only antigen
    associated (p < 0.01) with the absence of lung and presence of bone
    metastases, while it did not impact on overall survival or response to
    therapy. A1 (p < 0.0001, p < 0.002) and B8 (p < 0.009, p < 0.04) alleles
    were more frequently expressed in responding patients than expected from
    the normal distribution in Caucasians and that observed in
    non-responding patients, respectively. The HLA analysis of patients
    achieving a durable complete remission showed a significantly higher
    frequency of expression of the A1 and B8 antigens and furthermore of the
    B14 antigen (p < 0.05).

    Conclusions: Our data underline the pivotal role of the MHC complex in
    controlling and regulating the cellular immune response in renal cell
    cancer. We could identify HLA antigens, which correlate with response
    to cytokine-treatment, with a long-lasting effective tumor control and
    prolonged overall survival." 
}
